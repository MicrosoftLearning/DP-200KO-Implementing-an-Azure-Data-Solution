---
lab:
    title: 'Azure Data Factory를 사용하여 데이터 이동 오케스트레이션'
    module: '모듈 7: Azure Data Factory를 사용하여 데이터 이동 오케스트레이션'
---

# DP 200 - 데이터 플랫폼 솔루션 구현
# 랩 7 - Azure Data Factory를 사용하여 데이터 이동 오케스트레이션

**예상 시간**: 45분

**전제 조건**: 이 랩에 대한 사례 연구가 이미 밝혀졌던 것으로 가정합니다. 모듈 1의 콘텐츠 및 랩은 다음과 같은 것으로 가정합니다. 데이터 엔지니어용 Azure도 완료되었습니다.

**랩 파일**: 이 랩의 파일은 _Allfiles\Labfiles\Starter\DP-200.7_폴더에 위치해있습니다.

## 랩 개요

이 모듈에서 학생들은 다양한 데이터 플랫폼 기술의 데이터 이동을 오케스트레이션하는 데 Azure Data Factory를 사용하는 방법을 알아봅니다. 기술의 기능을 설명하고 SQL Database에서 데이터를 수집하여 SQL Data Warehouse에 데이터를 로드하는 엔드-엔드 데이터 파이프라인을 설정할 수 있습니다. 학생은 컴퓨팅 리소스를 호출하는 방법도 시연합니다.

## 랩 목표
  
이 랩을 완료하면 다음과 같은 것들을 수행할 수 있습니다.

1. 데이터 스트림 및 이벤트 처리 설명
1. Event Hubs를 사용하여 데이터 수집
1. Stream Analytics 작업을 사용하여 데이터 처리

## 시나리오
  
데이터 웨어하우스의 초기 채우기를 Azure SQL Data Warehouse로 수행한 후 정보 서비스 부서는 이 프로세스를 자동화하려고 합니다. Azure SQL Database에서 데이터 이동을 자동화할 수 있는 솔루션을 개발하는 과정에서 정보 서비스 부서를 지원하도록 요청을 받았습니다.

솔루션은 Azure SQL Data Warehouse에서 이름이 동일한 차원 테이블로 작용하는 [SalesLT].[ProductCategory] 및 [SalesLT].[ProductDescription] 트랜잭션 테이블을 완전히 복사할 수 있어야 합니다. 또한 솔루션은 Azure Data Factory를 데이터 이동의 오케스트레이터로 사용하여 MPP(대규모 병렬 처리) 시스템에 로드하는 모범 사례를 따라야 합니다.

또한 데이터 과학자는 Azure Data Factory에서 Azure Databricks를 호출할 수 있는지 확인하도록 요청했습니다. 이를 위해 Azure Databricks를 컴퓨팅 리소스로 호출하는 간단한 개념 증명 Data Factory 파이프라인을 만듭니다.

이 실습의 끝 부분에서는 다음을 수행할 수 있을 것입니다.

1. Azure Data Factory 작동 방식 설명
1. Azure Data Factory 구성 요소
1. Azure Data Factory 및 Databricks

## 연습 1: Azure Data Factory 작동 방식 설명

예상 시간: 15분

그룹 연습
  
이 연습의 주요 작업은 다음과 같습니다.

1. 사례 연구 및 시나리오에서 AdventureWorks의 데이터 스트림 수집 기술과 소셜 미디어 분석 요구 사항을 완료하기 위해 데이터 엔지니어로서 수행할 높은 수준의 작업을 식별합니다.

1. 강사는 그룹과 결과에 대해 논의합니다.

### 작업 1: AdventureWorks의 데이터 요구 사항 및 구조를 식별합니다.

1. 랩 가상 머신에서, **Microsoft Word**를 시작하고up the file **Allfiles\Labfiles\Starter\DP-200.6** 폴더의 **DP-200-Lab06-Ex01.docx** 파일을 엽니다.

1. 한 그룹으로 **10분**을 할애하여 데이터 요구 사항과 사례 연구 문서 내에서 그룹이 식별한 데이터 구조에 대해 논의하고 이를 나열합니다.

### 작업 2: 강사와 조사 결과에 대해 토론합니다.

1. 강사는 결과를 논의하기 위해 그룹 토론을 중지합니다.

> **결과**: 이 연습을 완료한 후 데이터 스트리밍 수집 테이블과 소셜 미디어 분석 요구 사항을 완료하기 위해 데이터 엔지니어로 수행할 상위 수준의 작업을 보여 주는 Microsoft Word 문서를 만들었습니다.

## 연습 2: Azure Data Factory 구성 요소
  
예상 시간: 15분

개인 연습
  
이 연습의 주요 작업은 다음과 같습니다.

1. Data Factory 인스턴스를 만듭니다.

1. 입력 연결 서비스 만들기

1. 입력 데이터 집합 정의

1. 출력 연결 서비스 만들기

1. 출력 데이터 집합 정의

1. SQL Data Warehouse에 최적화할 설정 완료

1. 파이프라인 실행 모니터링

1. Azure Data Factory 구성 요소 확인

1. 데이터 출력 확인

### 작업 1: Data Factory 인스턴스 만들기

1. Microsoft Edge에서, Azure Portal 탭으로 이동하여 **+ 리소스 생성하기**를 선택하고, **팩터리**를 입력한 다음, 결과 검색에서 **Data Factory**를 선택합니다. 그런 다음 **만들기**를 선택합니다.

1. 다음 옵션을 사용하여 새 Data Factory를 생성한 다음, **만들기**를  클릭합니다.
    - **이름**: xx-data-factory, 여기에서 xx는 이니셜을 나타냄
    - **구독**: 구독
    - **자원 그룹**: awrgstudxx
    - **버전**: V2
    - **장소**: 가장 가까운 위치를 선택합니다.
    - 다른 옵션을 기본 설정으로 남겨 둡니다.

    > **참고**: Data Factory를 만드는 데 약 1분이 걸립니다.

### 작업 2: 입력 연결 서비스 만들기

1. Azure Portal에서 Azure Data Factory 설치가 성공적으로 완료되었음을 명시하고 **리소스로 이동**을 클릭하는 메시지가 반환됩니다.

1. **xx-data-factory** 화면에서 **작성자 및 모니터를**클릭합니다. Azure Data Factory 솔루션을 작성하기 위해 다른 탭이 열립니다.

1. **시작하기** 페이지에서 **데이터 복사** 타일을 선택하여 데이터 복사 도구를 시작합니다.

1. **속성** 페이지에서 작업 이름 필드에 대해 **CopyFromSQLToSQLDW**를 지정하고 다음을 선택합니다.

1. **소스 데이터 저장소** 페이지에서 다음 단계를 완료한 다음, **다음**을 클릭합니다.
    - **+ 새 연결 만들기**를 클릭합니다.
    - 갤러리에서 **Azure SQL Database**를 선택하고 **계속**을 선택합니다.
    - **새로 연결된 서비스(Azure SQL Database)** 페이지에서 서버 이름 **sqlservicexx**와 드롭다운 목록의 데이터베이스 이름인 **DeptDatabasesxx**를 선택하고 사용자 이름과 암호를 지정합니다. **연결 테스트** 연결을 클릭하여 설정의 유효성을 검사한 다음, **완료**를 클릭합니다.

### 작업 3: 입력 데이터 집합 정의

1. **기존 테이블**아래 데이터 복사 마법사의 "데이터를 복사하거나 사용자 지정 쿼리를 사용할 테이블 선택"에서 [SalesLT].[ProductCategory] 및 [SalesLT].[ProductDescription] 바로 옆의 확인란을 클리하고 **다음**을 클릭합니다.

### 작업 4: 출력 연결 서비스 만들기

1. **대상 데이터 저장소** 페이지에서 다음 단계를 완료한 다음, **다음**을 클릭합니다.
    - **+ 새 연결 만들기**를 클릭합니다.
    - 갤러리에서 **Azure SQL Data Warehouse**를 선택하고 **계속**을 선택합니다.
    - **새 연결 서비스(Azure SQL Data Warehouse)** 페이지에서, 서버 이름 **sqlservicexx**와 드롭다운 목록의 데이터 베이스 이름 **DWDB**를 선택하고 사용자 이름과 암호를 지정합니다. **연결 테스트** 연결을 클릭하여 설정의 유효성을 검사한 다음, **완료**를 클릭합니다.

### 작업 5: 출력 데이터 집합 정의

1. 데이터 복사 마법사의 **소스** 아래, "테이블 매핑"에서 데이터 복사 마법사에서 대상이 동일한 이름을 갖는 [SalesLT].[ProductCategory] 및 [SalesLT].[ProductDescription]이 확인란 바로 옆에 있도록 보장합니다. **다음**을 클릭합니다.

1. 데이터 복사 마법사의 "열 매핑"에서 원본과 대상 사이의 열의 직접 복사본이어야 하는 **열 매핑**을 읽습니다. **다음**을 클릭합니다.

### 작업 6: SQL Data Warehouse에 최적화할 설정 완료

1. 데이터 복사 마법사의 **성능 설정** 아래 "설정"에서 **준비 활성화** 옆의 확인란이 선택되어 있는지 확인합니다.

1. "계정 연결 서비스 준비 바로 옆에서, **+ 신규**를 클릭하고 새 연결 서비스 페이지에서 스토리지 계정 **awsastudxx**를 선택하고 **종료**를 선택합니다.

1. **고급 설정** 섹션에서 **PolyBase 허용**을 선택해야 합니다. 

1. **고급 설정** 섹션에서, **유형 사용 기본 옵션**을 선택 취소하고 **다음**을 클릭합니다.

1. **요약** 페이지에서 설정을 검토하고 **다음**을 선택합니다. 

### 작업 7: 파이프라인 실행 모니터링

1. 배포 페이지에서 모니터를 선택하여 파이프라인 작업을 **모니터링**합니다.

1. 왼쪽의 모니터 탭이 자동으로 선택됩니다. **작업** 열에는 **활동 실행 세부 정보를 보고 **파이프라인을 다시 실행하는 링크가 포함되어 있습니다.

1. 파이프라인 실행과 연결된 활동 실행을 보려면 작업 열에서 **활동 실행 보기** 링크를 선택합니다. 파이프라인 실행 보기로 다시 전환하려면 맨 위에 있는 **파이프라인 **링크를 선택합니다. **새로 고침**을 선택하여 목록을 새로 고칩니다.

1. 각 복사 활동에 대한 실행 세부 정보를 모니터링하려면 활동 모니터링 보기의 작업에서 **세부 정보** 링크를 선택합니다. 원본에서 싱크로 복사된 데이터 볼륨, 데이터 처리량, 해당 기간이 있는 실행 단계 및 사용된 구성과 같은 세부 정보를 모니터링할 수 있습니다.

### 작업 8: Azure Data Factory 구성 요소 확인

1. **작성자 및 모니터** 화면에서 **작성자** 아이콘을 클릭합니다.

1. **팩터리 리소스**에서 방금 실행한 복사 마법사에 정의된 대로 **파이프라인 1개**와 **데이터 집합 2개**임을 확인합니다.

1. Microsoft Edge에서 Azure Data Factory 탭 닫기

### 작업 9: 데이터 출력 확인

1. 윈도우 데스크톱에서 **시작**을 클릭하고 **"SQL Server"**를 입력한 다음 **MIcrosoft SQL Server 관리 스튜디오 17**을 클릭합니다.

1. **서버에 연결** 대화 상자에서, 다음 세부 사항을 기입합니다.
    - 서버 이름: **sqlservicexx.database.windows.net**
    - 인증: **SQL Server 인증**
    - 사용자 이름: **xxsqladmin**
    - 암호: **P@ssw0rd**

1. **서버 연결**대화 상자에서 **연결**을 클릭합니다.

1. 데이터베이스 **DWDB**를 확장한 다음 **테이블**을 확장하고 [SalesLT].[ProductCategory] 및 [SalesLT].[ProductDescription]가 존재하는지 확인합니다.

1. **Server Management Studio** 닫기

> **결과**: 이 연습을 완료한 후 마법사를 사용하여 Azure SQL Database에서 Azure SQL Data Warehouse로 데이터를 이동하는 Azure Data Factory 구성 요소를 만들었습니다. 사용된 구성 요소와 데이터가 데이터 웨어하우스에 로드되었는지 확인했습니다.

## 연습 3: Azure Data Factory 및 Databricks
  
예상 시간: 15분

개인 연습
  
이 연습의 주요 작업은 다음과 같습니다.

1. Databricks 액세스 토큰을 생성합니다.

1. Databricks 노트 생성

1. 연결된 서비스 만들기

1. Databricks 노트 활동을 사용하는 파이프라인을 만듭니다.

1. 파이프라인 실행을 트리거합니다.

### 작업 1: Databricks 액세스 토큰을 생성합니다.

1. Azure Portal에서 **리소스 그룹**을 선택하고 **awrgstudxx**를 클릭한 다음, **awdbwsstudxx**을 클릭합니다. 여기에서 xx는 이름 이니셜입니다.

1. **작업 영역 시작**을 클릭합니다.

1. Databricks작업 영역의 오른쪽 상단 모서리에 있는 사용자 **프로필 아이콘**을 클릭합니다.

1. **사용자 설정**을 클릭합니다.

1. 토큰 액세스 탭으로 이동하여 **새 토큰 생성** 버튼을 클릭합니다.

1. "ADF 통합용" **주석**에 설명을 입력하고 10일의 **수명**을 설정하고 **생성**을 클릭합니다.

1. 생성된 토큰을 복사하여 메모장에 저장한 다음 **완료를** 클릭합니다.

### 작업 2: Databricks 노트 생성

1. 화면 왼쪽에서 **작업 영역 **아이콘을 클릭하고 작업 영역이라는 단어 옆의 화살표를 클릭한 다음 **만들기**를 클릭한 다음, **폴더**를 클릭합니다. 폴더 이름을 **adftutorial**로 지정하고 **폴더 만들기**를 클릭합니다. adftutorial 폴더가 작업 영역에 나타납니다.

1. adftutorial 옆의 드롭다운 화살표를 클릭하고 **만들기**를 클릭한 다음, **노트**를 클릭합니다.

1. 노트 만들기 대화 상자에서 **mynotebook**의 이름을 입력하고 언어가 Python **Python**을 명시하는지 확인한 다음, **만들기**를 클릭합니다. 내 노트가 있는 노트가 나타납니다.

1. 새로 만든 노트 "mynotebook"에서 다음 코드를 추가합니다.

    ```Python
    # 매개 변수를 활용하고 매개 변수를 인쇄하기 위한 위젯 만들기

    dbutils.widgets.text("input", "","")
    dbutils.widgets.get("input")
    y = getArgument("input")
    print ("Param -\'input':")
    print (y)
    ```

    > **노트** 경로는 **/adftutorial/mynotebook**입니다.

### 작업 3: 연결된 서비스 만들기

1. Microsoft Edge에서 포털 탭을 클릭합니다. Azure Portal에서 Azure Data Factory를 반환합니다.

1. **xx-data-factory** 화면에서 **작성자 및 모니터를**클릭합니다. Azure Data Factory 솔루션을 작성하기 위해 다른 탭이 열립니다.

1. 화면 왼쪽에서 **작성자**아이콘을 클릭합니다. 이렇게 하면 Data Factory 디자이너가 열립니다.

1. 화면 하단에서 **연결**을 클릭한 다음 **+ 신규**를 클릭합니다.

1. **새 연결 서비스**의 화면 상단에서 계산을 클릭한 다음  **컴퓨팅**을 클릭하고 **Azure **Databricks를 클릭한 다음, **계속**을 클릭합니다.

1. **새 연결 서비스(Azure Databricks)** 화면에서 다음 세부 정보를 입력하고 **완료**를 클릭합니다.
    - **이름**: xx_dbls, xx는 이니셜입니다
    - **Databricks 작업 영역**: awdbwsstudxx, 여기에서 xx는 이니셜입니다.
    - **클러스터 선택**: 기존 사용
    - **도메인/지역**: 채워야 합니다.
    - **액세스 토큰**: 메모장에서 액세스 토큰을 복사하여 이 필드에 붙여넣기
    - **기존 클러스터에서 선택**: awdbclstudxx, 여기에서 xx는 이니셜입니다
    - 다른 옵션을 기본 설정으로 남겨 둡니다.

    > **참고**: 완료를 클릭하면 xx_dbls가 생성된 **작성자 및 모니터** 화면으로 돌아가고 이전 사용 설명에서 만든 다른 연결 서비스가 표시됩니다.

### 작업 5: Databricks 노트 활동을 사용하는 파이프라인을 만듭니다.

1. 화면 왼쪽에 있는 팩터리 리소스 아래에서 **+** 아이콘을 클릭한 다음, **파이프라인**을 클릭합니다. 이렇게 하면 파이프라인 디자이너가 있는 탭이 열립니다.

1. 파이프라인 디자이너의 하단에서 매개 변수 탭을 클릭한 다음 **+ 신규**를 클릭합니다.

1. **name** 이름이 있는 매개 변수 만들기(**스트링 타입 포함)**

1. **활동** 메뉴에서 **Databricks를** 확장합니다.

1. **노트**를 캔버스로 드래그합니다.

1. 하단에 있는 **Notebook1** 창의 속성에서 다음 단계를 완료합니다.
    - **Azure Databricks** 탭으로 이동합니다.
    - 이전 절차에서 만든 **xx_dbls**를 선택합니다.

    - **설정** 탭으로 전환하고 노트 경로에 **/adftutorial/mynotebook**을 입력합니다.
    - **기본 매개 변수**를 확장한 다음, **+ 신규**를 클릭합니다.
    - **입력** 이름(**@pipeline().parameters.name** 값 포함)을 갖는 매개 변수를 만듭니다.

1. **Notebook1**에서 템플릿으로 저장 버튼 옆의 **유효성 검사**를 클릭합니다. 화면 오른쪽에 "파이프라인의 유효성이 검사되었습니다.
오류가 발견되지 않았습니다."라 명시된 창이 나타납니다. 창을 닫으려면 >>를 클릭합니다.

1. **모두 게시**를 클릭하여 연결된 서비스 및 파이프라인을 게시합니다.

    > **참고**: 배포가 성공했다는 메시지가 나타납니다.

### 작업 6: 파이프라인 실행 트리거

1. **Notebook1**에서 **트리거 추가**를 클릭하고 디버그 버튼 옆의 **지금 트리거**를 클릭합니다.

1. **파이프라인 실행** 대화 상자는 이름 매개 변수를 요청합니다. 여기에서 **/path/filename**을 매개 변수로 사용합니다. 종료를 클릭합니다. 캔버스의 Notebook1 활동 위에 빨간색 원이 나타납니다.

### 작업 7: 파이프라인 모니터링

1. 화면 왼쪽에서 **모니터** 탭을 클릭합니다. 파이프라인 실행이 표시되는지 확인합니다. 노트가 실행되는 Databricks 작업 클러스터를 만드는 데 약 5~8분이 걸립니다.

1. 주기적으로 **새로 고침**을 선택하여 파이프라인 실행 상태를 확인합니다.

1. 파이프라인 실행과 연결된 활동 실행을 보려면 **작업 열**에서 **활동 실행 보기**를 선택합니다.

### 작업 8: 출력 확인

1. Microsoft Edge에서 **mynotebook - Databricks** 탭을 클릭합니다. 

1. **Azure Databricks** 작업 영역에서 **클러스터**를 클릭하면 작업 상태가 실행 중, 작동 중 또는 종료됨으로 볼 수 있습니다.

1. **awdbclstudxx** 클러스터를 클릭한 다음 **이벤트 로그**를 클릭하여 활동을 봅니다.

    > **참고**: 파이프라인 실행을 트리거한 시간으로 **시작**되는 이벤트 유형을 볼 수 있어야 합니다.